{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "\n",
    "print\n",
    "print \"checking for nltk\"\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    print \"you should install nltk before continuing\"\n",
    "\n",
    "print \"checking for numpy\"\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    print \"you should install numpy before continuing\"\n",
    "\n",
    "print \"checking for scipy\"\n",
    "try:\n",
    "    import scipy\n",
    "except:\n",
    "    print \"you should install scipy before continuing\"\n",
    "\n",
    "print \"checking for sklearn\"\n",
    "try:\n",
    "    import sklearn\n",
    "except:\n",
    "    print \"you should install sklearn before continuing\"\n",
    "\n",
    "print\n",
    "print \"downloading the Enron dataset (this may take a while)\"\n",
    "print \"to check on progress, you can cd up one level, then execute <ls -lthr>\"\n",
    "print \"Enron dataset should be last item on the list, along with its current size\"\n",
    "print \"download will complete at about 423 MB\"\n",
    "\n",
    "\n",
    "\n",
    "print\n",
    "print \"unzipping Enron dataset (this may take a while)\"\n",
    "import tarfile\n",
    "import os\n",
    "os.chdir(\"/Users/shenyue/Downloads/ud120-projects-master\")\n",
    "tfile = tarfile.open('enron_mail_20150507.tgz', \"r\")\n",
    "tfile.extractall(\".\")\n",
    "\n",
    "print \"you're ready to go!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "import os\n",
    "os.chdir(\"/Users/shenyue/Downloads/ud120-projects-master\")\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"/Users/shenyue/Downloads/ud120-projects-master/tools/word_data.pkl\", authors_file=\"/Users/shenyue/Downloads/ud120-projects-master/tools/email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n",
      "training time: 2.35 s\n",
      "testing time: 0.229 s\n",
      "0.973265073948\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from time import time\n",
    "\n",
    "sys.path.append(\"/Users/shenyue/Downloads/ud120-projects-master/tools/\")\n",
    "features_train, features_test, labels_train, labels_test = preprocess()\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf=GaussianNB()\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train,labels_train)\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "t0 = time()\n",
    "pred=clf.predict(features_test)\n",
    "print \"testing time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(labels_test,pred)\n",
    "print accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
